{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of public_sentence_similarity_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KarNRHGcw0N",
        "colab_type": "text"
      },
      "source": [
        "<center> <h1> Transfer Learning: Sentence Similarity Task </h1> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppQypEhWkQMw",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pg-lLhROCKI",
        "colab_type": "code",
        "outputId": "21e7ab02-4d81-4285-dd78-8745b3e321ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-transfer-learning-hw/{dev,test,train}.tsv -q -nc\n",
        "!head train.tsv "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Can you suggest a best budget phone below 15k?\tWhat is the best phone I can buy under the price of 15000?\t1\n",
            "How can I make a disabled or accident-prone spouse feel useful and respected?\tHow does it feel to suddenly realize that your children are the product of a broken home because of you and/or your spouse?\t0\n",
            "I had an overdraft in Wells Fargo US! What will happen if I don't pay it?\tHow do you stop payment on a Wells Fargo check?\t0\n",
            "What are the best places to visit this December in India?\tWhat will be the best place to visit in December in India?\t1\n",
            "What should I do if I want to renew an expired driver's license, but had an accident while the license was expired? (India)\tIn what US state is it easy to get a driver's license?\t0\n",
            "How/why did Stanford develop such a strong entrepreneurial culture? Why doesn't UC Berkeley have such a strong entrepreneurial culture in comparison?\tHow strong is the startup culture in Berkeley?\t0\n",
            "How much weight can a honey bee lift?\tHow much force in Newton required to lift the water from hand pump?\t0\n",
            "What is the scariest experience you have had on a computer?\tWhat are some of the scariest experiences that you've had?\t0\n",
            "When is the best time to take apple cider vinegar? \tHow do I take Apple cider vinegar and when is the best time?\t1\n",
            "How much does autopilot influence a plane? In which phase is it most used?\tHow much does the autopilot do in an airliner?\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrFuULk1kj8a",
        "colab_type": "text"
      },
      "source": [
        "# Pretrained Models\n",
        "(https://github.com/huggingface/transformers) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igc-NhlYOc7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-transformers -q "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCrnza0SQcnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "from pytorch_transformers import DistilBertModel as BertModel\n",
        "from pytorch_transformers import DistilBertTokenizer as BertTokenizer\n",
        "random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "torch.cuda.set_device(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Vcp-d0nxVd",
        "colab_type": "text"
      },
      "source": [
        "# Data Reader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulIMqtG1M_6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPL_SYMS = ['<PAD>','<BOS>', '<EOS>', '<UNK>']\n",
        "\n",
        "\n",
        "class STSCorpus(object):\n",
        "  def __init__(self,\n",
        "              file,\n",
        "              vocab=None,\n",
        "              cuda=False,\n",
        "              batch_size=1, bert_format=0):\n",
        "    self.bert_format = bert_format\n",
        "    if self.bert_format == 0:\n",
        "      self.bert_tokenizer = None\n",
        "      self.max_vocab = 64000\n",
        "    else:\n",
        "      self.bert_tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "      self.max_vocab = self.bert_tokenizer.vocab_size\n",
        "    self.max_size = 0\n",
        "    self.batch_size = batch_size\n",
        "    self.vocab = self.make_vocab(file, vocab)\n",
        "    self.idx2vocab = self.make_idx2vocab(self.vocab)\n",
        "    self.data = self.numberize(file, self.vocab, cuda)\n",
        "    self.batch_data = self.batchify()\n",
        "    self.data_size = len(self.batch_data)\n",
        "\n",
        "  def batchify(self,):\n",
        "    self.batch_data = []\n",
        "    curr_batch = []\n",
        "    max_x1, max_x2 = 0, 0\n",
        "    for x1, x2, y in self.data:\n",
        "      if len(curr_batch) < self.batch_size:\n",
        "        curr_batch.append((x1, x2, y))\n",
        "        max_x1 = max(max_x1, x1.shape[1])\n",
        "        if self.bert_format == 0:\n",
        "          max_x2 = max(max_x2, x2.shape[1]) \n",
        "      else:\n",
        "        \n",
        "        _x1, _x2, _y = zip(*curr_batch)\n",
        "        \n",
        "        \n",
        "        if self.bert_format == 0:\n",
        "          _x1 = [torch.cat((torch.zeros(1, max_x1 - i.shape[1]).type_as(i), i), dim=1) for i in _x1]\n",
        "          batch_x1 = torch.cat(_x1, dim=0)\n",
        "          _x2 = [torch.cat((torch.zeros(1, max_x2 - i.shape[1]).type_as(i), i), dim=1) for i in _x2]\n",
        "          batch_x2 = torch.cat(_x2, dim=0) if _x2[0] is not None else None\n",
        "        else:\n",
        "          _x1 = [torch.cat((i, torch.zeros(1, max_x1 - i.shape[1]).type_as(i)), dim=1) for i in _x1]\n",
        "          batch_x1 = torch.cat(_x1, dim=0)\n",
        "          batch_x2 = None\n",
        "        batch_y = torch.cat(_y, dim=0)\n",
        "        self.batch_data.append((batch_x1, batch_x2, batch_y))\n",
        "        curr_batch = []\n",
        "        max_x1, max_x2 = 0, 0\n",
        "\n",
        "    if len(curr_batch) > 0:\n",
        "      print(len(self.batch_data),  max_x1, max_x2)\n",
        "      _x1, _x2, _y = zip(*curr_batch)\n",
        "      \n",
        "      \n",
        "      if self.bert_format == 0:\n",
        "        _x1 = [torch.cat((torch.zeros(1, max_x1 - i.shape[1]).type_as(i), i), dim=1) for i in _x1]\n",
        "        batch_x1 = torch.cat(_x1, dim=0)\n",
        "        _x2 = [torch.cat((torch.zeros(1, max_x2 - i.shape[1]).type_as(i), i), dim=1) for i in _x2]\n",
        "        batch_x2 = torch.cat(_x2, dim=0) if _x2[0] is not None else None\n",
        "      else:\n",
        "        _x1 = [torch.cat((i, torch.zeros(1, max_x1 - i.shape[1]).type_as(i)), dim=1) for i in _x1]\n",
        "        batch_x1 = torch.cat(_x1, dim=0)\n",
        "        batch_x2 = None\n",
        "      batch_y = torch.cat(_y, dim=0)\n",
        "      self.batch_data.append((batch_x1, batch_x2, batch_y))\n",
        "    return self.batch_data\n",
        "\n",
        "  def numberize(self, txt, vocab, cuda=False):\n",
        "    data = []\n",
        "    max_size = 0\n",
        "    with open(txt, 'r', encoding='utf8') as corpus:\n",
        "      for l in corpus:\n",
        "        l1, l2, y = l.split('\\t')\n",
        "        y = torch.Tensor([[float(y)]]).float()\n",
        "        if self.bert_format == 0:\n",
        "          d1 = [vocab['<BOS>']] + [vocab.get(t, vocab['<UNK>']) for t in l1.strip().split()] + [vocab['<EOS>']]\n",
        "          d1 = torch.Tensor(d1).long()\n",
        "          d1 = d1.unsqueeze(0) \n",
        "          d2 = [vocab['<BOS>']] + [vocab.get(t, vocab['<UNK>']) for t in l2.strip().split()] + [vocab['<EOS>']]\n",
        "          d2 = torch.Tensor(d2).long()\n",
        "          d2 = d2.unsqueeze(0) \n",
        "          max_size = max(d1.shape[1], d2.shape[1], max_size)\n",
        "          if cuda:\n",
        "            d1 = d1.cuda()\n",
        "            d2 = d2.cuda()\n",
        "            y = y.cuda()\n",
        "        elif self.bert_format == 1:\n",
        "          _d1 = torch.Tensor(self.bert_tokenizer.encode(\"[CLS] \" + l1 + \" [SEP]\")).long()\n",
        "          _d2 = torch.Tensor(self.bert_tokenizer.encode(\" \" + l2 + \" [SEP]\")).long()\n",
        "          d = torch.cat([_d1, _d2], dim=0).unsqueeze(0)\n",
        "          max_size = max(d.shape[1], max_size)\n",
        "          if cuda:\n",
        "            d1 = d.cuda()\n",
        "            d2 = None\n",
        "            y = y.cuda()\n",
        "        else:\n",
        "          pass\n",
        "        data.append((d1, d2, y))\n",
        "    self.max_size = max_size\n",
        "    return data\n",
        "\n",
        "  def make_idx2vocab(self, vocab):\n",
        "    if vocab is not None:\n",
        "      idx2vocab = {v: k for k, v in vocab.items()}\n",
        "      return idx2vocab\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def make_vocab(self, txt, vocab):\n",
        "    if vocab is None and txt is not None:\n",
        "      vc = {}\n",
        "      for line in open(txt, 'r', encoding='utf-8').readlines():\n",
        "        x1, x2, y = line.strip().split('\\t')\n",
        "        for w in x1.split() + x2.split():\n",
        "          vc[w] = vc.get(w, 0) + 1\n",
        "      cv = sorted([(c, w) for w, c in vc.items()], reverse=True)\n",
        "      cv = cv[:self.max_vocab]\n",
        "      _, v = zip(*cv)\n",
        "      v = SPL_SYMS + list(v)\n",
        "      vocab = {w: idx for idx, w in enumerate(v)}\n",
        "      return vocab\n",
        "    else:\n",
        "      return vocab\n",
        "\n",
        "  def get(self, idx):\n",
        "    return self.batch_data[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvu6zsZ-pgZA",
        "colab_type": "text"
      },
      "source": [
        "Creating train, dev and test data objects. (with `bert_format=0`) and places the data on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk05u089ZmVC",
        "colab_type": "code",
        "outputId": "7ded042f-ad7a-4167-e3e3-c4936075fc87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_corpus = STSCorpus(file='train.tsv',\n",
        "                         cuda=True,\n",
        "                         batch_size=32, \n",
        "                         bert_format=0)\n",
        "dev_corpus = STSCorpus(file='dev.tsv', vocab=train_corpus.vocab,\n",
        "                       cuda=True,\n",
        "                       batch_size=32, \n",
        "                       bert_format=0)\n",
        "test_corpus = STSCorpus(file='test.tsv', vocab=train_corpus.vocab,\n",
        "                        cuda=True,\n",
        "                        batch_size=1,\n",
        "                        bert_format=0)\n",
        "print(train_corpus.data_size, dev_corpus.data_size, test_corpus.data_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1212 19 15\n",
            "151 30 23\n",
            "1213 152 2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RxnLgrDybnZ",
        "colab_type": "code",
        "outputId": "1786ebd1-9d41-4025-bfa5-ff5aae0e34a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(train_corpus.batch_data[0][:2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     1,    48,    17,  1022,     7,    23,\n",
            "          1199,   166,  1932,  6382,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             1,    13,    18,     9,    76,     7,  4012,    26, 45538,  2312,\n",
            "           144,  1052,    12, 33066,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     1,     9,   171,\n",
            "            32, 13296,     8, 14337, 18610, 46694,     5,    39,   149,    35,\n",
            "             9,    96,   468,   111,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     1,     5,    14,     4,    23,   241,     6,   249,\n",
            "            91,  2087,     8,    64,     2],\n",
            "        [    1,     5,    38,     9,    15,    35,     9,   105,     6, 11283,\n",
            "            32,  5460,  6182, 21845,   118,   171,    32, 12094,   178,     4,\n",
            "          1515,    75, 40327, 11019,     2],\n",
            "        [    0,     0,     1, 52732,    74,  2612,   856,   381,     7,  1591,\n",
            "         11767,  1964,    22,   304,  3749, 10951,    33,   381,     7,  1591,\n",
            "         11767,   389,     8, 23534,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     1,    13,    81,   334,    18,\n",
            "             7,  3994, 12008,  6545,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     1,     5,    10,     4,  3945,   519,    17,    33,\n",
            "           171,    19,     7,  1267,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     1,   164,    10,     4,    23,   107,     6,\n",
            "           120,  3076,  5823,  8059,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n",
            "            13,    81,    31, 17556,  3141,     7,  4129,   184,    99,  2064,\n",
            "            10,    20,    55,  1523,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     1,    49,   230,  7587,\n",
            "            52,    67,    12,   395,    45,    39,    25,   176,   367,    19,\n",
            "             4,  1156,    11,   624,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     1,   114,   604,  5505,     8,  4757,\n",
            "            16, 13629,  7802,  1752,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1,    13,   394,    10,    20,     6,    29,   104, 46702,\n",
            "            16,     4,  1474,  8742,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1,    27,    20,   123,     6,   245,   538,  4052,   219,\n",
            "            41,   619,    41, 17576,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1, 25186,     5,    14,     4,   928,     8,    99,  1240,\n",
            "         39095,   944,   128,  1182,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1,     5,    14,     4,    23,  3689,   193,    24,    18,\n",
            "           154,    28,  5240,  2544,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1,     5,    14,     4,    55,  1795,  1916,    16,    95,\n",
            "             6,   227,    21,   711,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1,     5,    31,    20,   144,    50,     6,    33,    32,\n",
            "         13326,   664,   261,  2058,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     1,    13,    10,     4,  7911,   209,\n",
            "         47467, 57150,  1360,  1107,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             1,    37,    58,    10,    97,  4158,     6,  1518, 55579, 47299,\n",
            "         51584, 57199,    26, 51588,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n",
            "            27,    20,   300,    16,    95,    26,    21,   633,     6,   213,\n",
            "           630,   372,    81, 14278,     2],\n",
            "        [    0,     0,     0,     0,     1,  3522,  5556,   505,    17,   110,\n",
            "          3412,    19,     7,   888,   977,   894,     8,  3930,  6732,  4224,\n",
            "             5,   251,    20,  2933,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     1,    13,    15,     9,    29,\n",
            "          9548, 44576,    93, 36628,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     1,     5,    14,     4,\n",
            "            55,   722,  1294,     9,   159,     8,   421,     6,   457,    21,\n",
            "           329,   255,    42,   914,     2],\n",
            "        [    0,     0,     1,     9,   105,     6,   157,     4, 24825, 52715,\n",
            "          4080, 27128, 12305, 15034, 48291,  5466, 19471,    71,    18,     9,\n",
            "           102,    20,     8,    64,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             1,   258,     7,  3242,  4228, 13163,  2946,    25,    54,    16,\n",
            "          3057, 22180,   594,  8204,     2],\n",
            "        [    0,     0,     1,    37,    14,     4,    55,  1416,  2055,  2729,\n",
            "             9,    38,   390,   153,     9,  1148,    12,    53,    15,     9,\n",
            "           390,   278,   179,   279,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n",
            "            22,    15,    40,   742,     6,   124,  2573,    84,    19,    73,\n",
            "           911,    80,  4000,   111,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1,   184, 28102, 36155,    11, 53112,    86, 36145,    11,\n",
            "          9636,  4046,    10,  1234,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     1,    37,    58,    10,   119,  6091,  8384,\n",
            "          2647,    26, 16302, 57680,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n",
            "             5,    14,   302,  1290,  3605,  2730,    47,   492,  1606,  1318,\n",
            "             6,     4,  3930, 42509,     2],\n",
            "        [    0,     0,     0,     0,     1,    13,    81,    31,    20,   360,\n",
            "            12,   259,    18,     9,  1810,     7, 31836,   903,    16,     7,\n",
            "           951,   373,  3271,  7378,     2]], device='cuda:0'), tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             1,     5,    10,     4,    23,   166,     9,    18,   157,   261,\n",
            "             4,   812,    11,  6383,     2],\n",
            "        [    1,    13,    31,    20,   144,     6,  1775,  2521,    24,    36,\n",
            "          1069,    14,     4,   835,    11,     7,  2817,   510,   428,    11,\n",
            "            17,  1544,    36,  7485,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     1,    13,    15,    17,   155,  2098,    19,\n",
            "             7, 14337, 18610,  7778,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1,     5,    39,    25,     4,    23,   341,     6,   249,\n",
            "             8,  2087,     8,    64,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1,   184,    45,   131,   532,    10,    20,   882,     6,\n",
            "            29,     7,  6182,  4483,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     1,    13,  1591,    10,     4,\n",
            "           700,   389,     8,  6829,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             1,    13,    81,  1333,     8, 12286,   603,     6,  3037,     4,\n",
            "           355,    34,   997, 11337,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     1,     5,    14,    30,    11,     4,  3945,\n",
            "          2285,    24,   499,  3467,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             1,    13,    15,     9,   120,   578,  5823,  3778,    12,    46,\n",
            "            10,     4,    23,   204,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     1,    13,    81,    31,     4, 17556,\n",
            "            15,     8,    32, 14272,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     1,    13,    39,\n",
            "             4,  1889,   534,    25,   710,    35,    44,    10,     7,   230,\n",
            "            52,    67,    12,   564,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1,     5,    14,     4,    23,   567,    28,   265,   662,\n",
            "            11,  5715,  1361,  8270,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1,    13,   394,    43,    20,    25,     6,    29,   104,\n",
            "             4,  1474,  1939, 46701,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n",
            "             9,   105,     6,    15,   538,  4052,    12, 44494,     8,     7,\n",
            "         27439,    27,    20,  1293,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     1, 25186,     5,    14,\n",
            "            30,    11,     4,   944, 44770,    24, 12240,  4409, 45917, 45794,\n",
            "            10,  1240,   356,  1182,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     1,     5,    14,    30,  3689,   193,    11,  1497,\n",
            "            90,     6,  5240, 34206,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     1,     5,    38,     9,    15,     6,\n",
            "           227,    21,  2418,   711,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     1,     5,    10,    20,    50,     6,\n",
            "            33,    32, 14209,  2340,     2],\n",
            "        [    0,     0,     1,   239, 26112,  1360,  2774,   208,   445,    10,\n",
            "         27903, 28323, 28035,    13,    18,     9,    29, 27898,  1153,   346,\n",
            "             6,   151,    16, 26562,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     1,    13,\n",
            "            15,     9,  4286,   861,     4,   799,   611,    11,     4, 47300,\n",
            "            10,  8705,    26, 51583,     2],\n",
            "        [    0,     0,     0,     1,     9,   165,    21,   633,   372,    81,\n",
            "            12,  7456,   213,   237, 21957, 42452,    13,    18,     9,   100,\n",
            "             6,   165,   237,  7047,     2],\n",
            "        [    0,     0,     0,     0,     1,  3522,  5556,   505,    17,   110,\n",
            "          3412,    19,     7,   888,   355,   894,     8,  3930,  6732,  4224,\n",
            "             5,   251,    20,  2933,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             1,    37,    10,     4,  5680,   766,  1560,    16, 22473,    11,\n",
            "          1536,    12,  1985,  1107,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             1,     5,  1327,  1294,    43,     9,   159,     6,   457,     7,\n",
            "         22533,   510,  3154,  1267,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     1,    13,    86, 13297,    31, 12305,\n",
            "         15034,    33,     8,    64,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n",
            "           258,     7,  3242, 13163,  4228,  3154,  2946,    25,    54,    16,\n",
            "          3057, 22180,   594,  8204,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     1,     5,    14,    30,    11,\n",
            "             4,    23,  2055,   993,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     1,    22,    15,\n",
            "            40,   124,    84,    19,    73,    24,    14,   298,     6,   102,\n",
            "           307,   372,    19,   506,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     1,    13,    86,  7818,    14,  4619,\n",
            "             8,    32,  2897, 14159,     2],\n",
            "        [    0,     0,     1,   170,  1012,   299,   168,  1973,   239,  1494,\n",
            "            10,  4840, 21772,    48,     9, 11805,    21,  1494,   212,    63,\n",
            "         31792,  4340,    26,  2932,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     1,     5,    14,   302,  1290,  3605,  2730,    47,   492,\n",
            "          1606,  1318,     6, 19584,     2],\n",
            "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             1,    13,    81,    31,    20,   360,     6,    29,    36,  7454,\n",
            "         21066,     8,  3271,  7378,     2]], device='cuda:0'))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGkd0qTUy09o",
        "colab_type": "code",
        "outputId": "feccab39-61b0-41cb-9cdc-f94032331bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "print( train_corpus.batch_data[0][2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ber1qV5pxJ_",
        "colab_type": "text"
      },
      "source": [
        "# Training Routine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1265VP3bBrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_cropus, dev_corpus, max_epochs):\n",
        "  sum_loss, sum_acc = 0., 0.\n",
        "  train_instances_idxs = list(range(train_corpus.data_size))\n",
        "  st = time.time()\n",
        "  for epoch_i in range(max_epochs):\n",
        "    sum_loss, sum_acc = 0., 0.\n",
        "    random.shuffle(train_instances_idxs)\n",
        "    model.train()\n",
        "    for i in train_instances_idxs:\n",
        "      x1, x2, y = train_corpus.get(i)\n",
        "      l, a = model.train_step(x1, x2, y)\n",
        "      sum_loss += l\n",
        "      sum_acc += a\n",
        "    print(f\"epoch: {epoch_i} time elapsed: {time.time() - st:.2f}\")\n",
        "    print(f\"train loss: {sum_loss/train_corpus.data_size:.4f} train acc: {sum_acc/train_corpus.data_size:.4f}\")\n",
        "    sum_loss, sum_acc = 0., 0.\n",
        "    model.eval()\n",
        "    for dev_i in range(dev_corpus.data_size):\n",
        "      x1, x2, y = dev_corpus.get(dev_i)\n",
        "      with torch.no_grad():\n",
        "        l, a = model(x1, x2, y)\n",
        "        sum_loss += l\n",
        "        sum_acc += a\n",
        "    print(f\"  dev loss: {sum_loss/dev_corpus.data_size:.4f}   dev acc: {sum_acc/dev_corpus.data_size:.4f}\")\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q62YvNKlp1VA",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Routine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPlEx7Akb7m8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, test_corpus):\n",
        "  print('Predictions:')\n",
        "  sum_acc = 0.0\n",
        "  model.eval()\n",
        "  for test_i in range(test_corpus.data_size):\n",
        "    x1, x2, y = test_corpus.get(test_i)\n",
        "    _, pred = model.predict(x1, x2)\n",
        "    sum_acc += (1 if pred.item() == y.item() else 0)\n",
        "  print(f\"Avg acc: {sum_acc/test_corpus.data_size:.4f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Sw26rjp4nn",
        "colab_type": "text"
      },
      "source": [
        "# 1. Baseline Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avT4K1diMcvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embedding_size,\n",
        "                 hidden_size,\n",
        "                 num_layers=1,\n",
        "                 dropout=0.1,\n",
        "                 max_grad_norm=5.0):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding_size = embedding_size\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        \n",
        "        if max(vocab_size,embedding_size ,hidden_size,num_layers) > 0:\n",
        "          self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "          self.rnn = torch.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "          self.output = torch.nn.Linear(2*hidden_size, 1)\n",
        "  \n",
        "          self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()))\n",
        "        else:\n",
        "          pass\n",
        "        \n",
        "        self.loss = torch.nn.BCELoss(reduction='mean')\n",
        "\n",
        "    def predict(self, x1, x2):\n",
        "\n",
        "        batch_size, seq_len = x1.shape\n",
        "        batch_size2, seq_len2 = x2.shape\n",
        "        assert batch_size == batch_size2\n",
        "        \n",
        "        emb_x1 = self.embedding(x1)\n",
        "        emb_x1 = self.dropout(emb_x1)\n",
        "\n",
        "        emb_x2 = self.embedding(x2)\n",
        "        emb_x2 = self.dropout(emb_x2)\n",
        "        \n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        h0 = h0.cuda()\n",
        "        c0 = torch.zeros(self.num_layers, batch_size2, self.hidden_size)\n",
        "        c0 = c0.cuda()\n",
        "\n",
        "        out1, _  = self.rnn(emb_x1, (h0, c0))\n",
        "        \n",
        "        out2, _ = self.rnn(emb_x2, (h0, c0))\n",
        "\n",
        "        final_hidden = torch.cat((out1[:,-1], out2[:,-1]), 1).view(batch_size, 2 * self.hidden_size)\n",
        "        \n",
        "        final_hidden = self.dropout(final_hidden)\n",
        "\n",
        "        soo = self.output(final_hidden)\n",
        "        out = torch.nn.functional.sigmoid(soo).view(batch_size, 1)\n",
        "        \n",
        "        pred = out.clone().detach()\n",
        "        pred[pred >= 0.5] = 1\n",
        "        pred[pred < 0.5] = 0\n",
        "        return out, pred\n",
        "\n",
        "    def forward(self, x1, x2, y):\n",
        "        out, pred = self.predict(x1, x2)\n",
        "        loss = self.loss(out, y)\n",
        "        \n",
        "        assert pred.shape == y.shape\n",
        "        acc = (pred == y).sum().item() / y.numel()\n",
        "        return loss, acc\n",
        "\n",
        "    def train_step(self, x1, x2, y):\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        _loss, acc = self(x1, x2, y) \n",
        "        _loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, self.parameters()),\n",
        "                                                   self.max_grad_norm)\n",
        "\n",
        "        if math.isnan(grad_norm):\n",
        "            print('skipping update grad_norm is nan!')\n",
        "        else:\n",
        "            self.optimizer.step()\n",
        "        loss = _loss.item()\n",
        "        return loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FjjFilmaxeT",
        "colab_type": "code",
        "outputId": "c3a85bee-5808-4b71-c43e-f4484a822609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "base_model = Classifier(vocab_size=len(train_corpus.vocab),\n",
        "                        embedding_size=1024,\n",
        "                        hidden_size=1024,\n",
        "                        num_layers=2)\n",
        "print(base_model, '\\ncontains', sum([p.numel() for p in base_model.parameters() if p.requires_grad]), 'parameters')\n",
        "base_model = base_model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (embedding): Embedding(61585, 1024)\n",
            "  (rnn): LSTM(1024, 1024, num_layers=2, batch_first=True)\n",
            "  (output): Linear(in_features=2048, out_features=1, bias=True)\n",
            "  (loss): BCELoss()\n",
            ") \n",
            "contains 79858689 parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z48mQ5YNeJGS",
        "colab_type": "code",
        "outputId": "e7bbc0cb-36fb-42a9-f2f0-7c421149a75a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "base_model = train(base_model, train_corpus, dev_corpus, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 time elapsed: 235.50\n",
            "train loss: 0.5644 train acc: 0.6982\n",
            "  dev loss: 0.5141   dev acc: 0.7448\n",
            "epoch: 1 time elapsed: 479.92\n",
            "train loss: 0.4040 train acc: 0.8171\n",
            "  dev loss: 0.5291   dev acc: 0.7542\n",
            "epoch: 2 time elapsed: 724.33\n",
            "train loss: 0.2190 train acc: 0.9117\n",
            "  dev loss: 0.6638   dev acc: 0.7574\n",
            "epoch: 3 time elapsed: 968.77\n",
            "train loss: 0.0927 train acc: 0.9649\n",
            "  dev loss: 0.9561   dev acc: 0.7357\n",
            "epoch: 4 time elapsed: 1213.21\n",
            "train loss: 0.0505 train acc: 0.9813\n",
            "  dev loss: 1.2827   dev acc: 0.7509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQRf1DcDTv1U",
        "colab_type": "code",
        "outputId": "00c290d9-0f1a-4b5a-f451-c4dde5a2f755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "evaluate(base_model, test_corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Avg acc: 0.7756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk-B5fNvu5pZ",
        "colab_type": "text"
      },
      "source": [
        "Creating train, dev and test data objects. (with `bert_format=1`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNePb0x4f5Dg",
        "colab_type": "code",
        "outputId": "99aacc1c-682c-43c8-c9ec-be3baa1ef6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_corpus = STSCorpus(file='train.tsv',\n",
        "                          cuda=True,\n",
        "                          batch_size=32, bert_format=1)\n",
        "dev_corpus = STSCorpus(file='dev.tsv', vocab=train_corpus.vocab,\n",
        "                        cuda=True,\n",
        "                        batch_size=32,bert_format=1)\n",
        "test_corpus = STSCorpus(file='test.tsv', vocab=train_corpus.vocab,\n",
        "                        cuda=True,\n",
        "                        batch_size=1,bert_format=1)\n",
        "print(train_corpus.data_size, dev_corpus.data_size, test_corpus.data_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1212 33 0\n",
            "151 51 0\n",
            "1213 152 2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVai29qau8tl",
        "colab_type": "text"
      },
      "source": [
        "# 2. BERT based Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eiSbAgFMfoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTClassifier(Classifier):\n",
        "    def __init__(self,\n",
        "                 dropout=0.1,\n",
        "                 max_grad_norm=5.0):\n",
        "        super().__init__(0, 0, 0, 0, dropout, max_grad_norm)\n",
        "        self.output = torch.nn.Linear(768, 1)\n",
        "\n",
        "        torch.nn.init.normal_(self.output.weight, mean=0, std=0.05)\n",
        "\n",
        "        self.bert_model = BertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=1e-5)\n",
        "\n",
        "    def predict(self, x1, x2=None):\n",
        "        assert x2 is None\n",
        "\n",
        "        this_bert = self.bert_model(x1)\n",
        "\n",
        "        first_step = this_bert[0]\n",
        "        so = self.output(first_step)[:, 0]\n",
        "        out = torch.nn.functional.sigmoid(so)  \n",
        "        \n",
        "        pred = out.clone().detach()\n",
        "        pred[pred >= 0.5] = 1\n",
        "        pred[pred < 0.5] = 0\n",
        "        return out, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5vTTWrcqDgB",
        "colab_type": "code",
        "outputId": "09c2c2ad-4816-41cd-ece6-efa54b03e7c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bert_model = BERTClassifier()\n",
        "bert_model = bert_model.cuda()\n",
        "print(bert_model, '\\ncontains', sum([p.numel() for p in bert_model.parameters() if p.requires_grad]), 'parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERTClassifier(\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (loss): BCELoss()\n",
            "  (output): Linear(in_features=768, out_features=1, bias=True)\n",
            "  (bert_model): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0): TransformerBlock(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (1): TransformerBlock(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (2): TransformerBlock(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (3): TransformerBlock(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (4): TransformerBlock(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (5): TransformerBlock(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ") \n",
            "contains 66363649 parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0erCS_yoqMlf",
        "colab_type": "code",
        "outputId": "eb94ef94-524e-462b-d58d-a033737aaeb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "bert_model = train(bert_model, train_corpus, dev_corpus, 3) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 time elapsed: 144.34\n",
            "train loss: 0.4085 train acc: 0.8082\n",
            "  dev loss: 0.3188   dev acc: 0.8612\n",
            "epoch: 1 time elapsed: 293.70\n",
            "train loss: 0.2862 train acc: 0.8793\n",
            "  dev loss: 0.3204   dev acc: 0.8681\n",
            "epoch: 2 time elapsed: 443.34\n",
            "train loss: 0.2207 train acc: 0.9113\n",
            "  dev loss: 0.3201   dev acc: 0.8701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbstmfEhckWl",
        "colab_type": "code",
        "outputId": "c3ff2b48-2f72-4b09-ddaf-e68b55201908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "evaluate(bert_model, test_corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Avg acc: 0.8692\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}