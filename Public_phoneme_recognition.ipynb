{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Public-phoneme-recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIccL5GzxhR2",
        "colab_type": "code",
        "outputId": "cb3d8a03-da9b-440f-ce36-123f62748761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "random.seed(1234)\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff234750a70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMBltyhptzju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/dev_feats.mat.npy\n",
        "!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/train_feats.mat.npy\n",
        "!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/test_feats.mat.npy\n",
        "!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/dev_labels.mat.npy\n",
        "!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/train_labels.mat.npy\n",
        "!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/test_labels.mat.npy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuBERKCMwFnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_labels(data):\n",
        "    label_dict = {}\n",
        "    for y in data:\n",
        "        label_dict[y] = label_dict.get(y, len(label_dict))\n",
        "    return label_dict\n",
        "\n",
        "\n",
        "def load_npy():\n",
        "    train_feats = np.load('train_feats.mat.npy', allow_pickle=True)\n",
        "    train_labels = np.load('train_labels.mat.npy', allow_pickle=True)\n",
        "    label_idx = get_labels(train_labels)\n",
        "    test_feats = np.load('test_feats.mat.npy', allow_pickle=True)\n",
        "    test_labels = np.load('test_labels.mat.npy', allow_pickle=True)\n",
        "    dev_feats = np.load('dev_feats.mat.npy', allow_pickle=True)\n",
        "    dev_labels = np.load('dev_labels.mat.npy', allow_pickle=True)\n",
        "    return label_idx, (train_feats, train_labels), (dev_feats, dev_labels), (test_feats, test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edcfjz_PwpSt",
        "colab_type": "code",
        "outputId": "339e67a1-3245-4c80-c328-70d549c44842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "label_dict, train, dev, test = load_npy()\n",
        "\n",
        "#Display the shape of the features and labels\n",
        "print(train[0].shape, len(train[1]))\n",
        "print(dev[0].shape, len(dev[1]))\n",
        "print(test[0].shape, len(test[1]))\n",
        "\n",
        "#Display the first 40 labels\n",
        "print(train[1][:300])\n",
        "#Dispplay the first 40 speech features\n",
        "print(train[0][:40])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200000, 39) 200000\n",
            "(10000, 39) 10000\n",
            "(10000, 39) 10000\n",
            "['sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil'\n",
            " 'sil' 'ax' 'ax' 'ax' 'ax' 'ax' 'ax' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
            " 's' 's' 's' 's' 's' 's' 's' 's' 'uw' 'uw' 'uw' 'uw' 'uw' 'uw' 'uw' 'uw'\n",
            " 'uw' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'f' 'f' 'f' 'f' 'f' 'f' 'f'\n",
            " 'f' 'f' 'f' 'f' 'f' 'ao' 'ao' 'ao' 'ao' 'ao' 'ao' 'r' 'r' 'r' 'r' 'r'\n",
            " 'ix' 'ix' 'ix' 'vcl' 'vcl' 'vcl' 'vcl' 'vcl' 'z' 'z' 'z' 'z' 'z' 'z' 'z'\n",
            " 'z' 'ae' 'ae' 'ae' 'ae' 'ae' 'ae' 'ae' 'ae' 'ae' 'ae' 'm' 'm' 'm' 'm'\n",
            " 'cl' 'cl' 'cl' 'p' 'p' 'p' 'p' 'uh' 'uh' 'uh' 'l' 'l' 'l' 'l' 'l' 'l' 'l'\n",
            " 'l' 'l' 'l' 'l' 'l' 'l' 'ax' 'ax' 'ax' 'ax' 'ax' 'ax' 'ax' 's' 's' 's'\n",
            " 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 'ix' 'ix' 'ix' 'ix' 'ix' 'cl'\n",
            " 'cl' 'cl' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'uw'\n",
            " 'uw' 'uw' 'uw' 'uw' 'uw' 'uw' 'uw' 'uw' 'ey' 'ey' 'ey' 'ey' 'ey' 'ey'\n",
            " 'ey' 'ey' 'ey' 'ey' 'sh' 'sh' 'sh' 'sh' 'sh' 'sh' 'sh' 'sh' 'sh' 'sh'\n",
            " 'sh' 'en' 'en' 'en' 'en' 'en' 'en' 'en' 'w' 'w' 'w' 'w' 'w' 'w' 'w' 'w'\n",
            " 'w' 'w' 'w' 'w' 'eh' 'eh' 'eh' 'er' 'er' 'er' 'er' 'er' 'f' 'f' 'f' 'f'\n",
            " 'f' 'f' 'f' 'f' 'f' 'f' 'f' 'f' 'f' 'f' 'aa' 'aa' 'aa' 'aa' 'aa' 'aa'\n",
            " 'aa' 'aa' 'aa' 'aa' 'aa' 'aa' 'aa' 'r' 'r' 'r' 'm' 'm' 'm' 'm' 'm' 'm'\n",
            " 'hh' 'hh' 'hh' 'hh' 'hh' 'hh' 'hh' 'hh' 'eh' 'eh' 'eh' 'eh' 'z' 'z' 'z'\n",
            " 'z' 'z' 'z' 'z' 'z' 'ax' 'ax' 'ax' 'ax' 'cl' 'cl' 'cl' 'p' 'p' 'p' 'p'\n",
            " 'p' 'p' 'p' 'p' 'p' 'p']\n",
            "[[-27.69561   -21.09042     1.654011  ...   0.3484999   0.768689\n",
            "    1.307443 ]\n",
            " [-28.63498   -24.11119     5.481146  ...  -0.7078144   0.1252752\n",
            "    1.01771  ]\n",
            " [-31.45311   -22.93973     2.537196  ...  -1.359528   -1.21996\n",
            "   -0.538246 ]\n",
            " ...\n",
            " [ 13.88244    -0.1123238  -9.3858    ...  -3.32069     1.63194\n",
            "   -0.6642456]\n",
            " [ 14.03307     1.899275   -9.866893  ...  -3.532823    2.024883\n",
            "   -1.48834  ]\n",
            " [ 14.1837      0.1750488  -8.502616  ...  -1.771908    2.117862\n",
            "   -1.812105 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD26HuE00XT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchify(data_feats, data_labels, batch_size, label_dict, window=5, to_cuda=False):\n",
        "    batched_tdata = []\n",
        "    curr_batch = []\n",
        "    fz = np.zeros((window, 39))\n",
        "    bz = np.zeros((window, 39))\n",
        "    fl = ['sil'] * window\n",
        "    bl = ['sil'] * window\n",
        "    data_labels = fl + data_labels.tolist() + bl\n",
        "    data_feats = np.concatenate((fz, data_feats, bz))\n",
        "    for i in range(window, len(data_labels) - window):\n",
        "        x = data_feats[i - window: i + window + 1]\n",
        "        y = data_labels[i]\n",
        "        tx = torch.Tensor(x).unsqueeze(0) # shape should be (1, 39, 2window)\n",
        "        ty = torch.Tensor([label_dict[y]]) # shape should be (1, 1)\n",
        "        if len(curr_batch) < batch_size:\n",
        "            #if y != 'sil':\n",
        "            curr_batch.append((tx, ty))\n",
        "        else:\n",
        "            _tx, _ty = zip(*curr_batch)\n",
        "            b_tx = torch.cat(_tx, dim=0)\n",
        "            b_ty = torch.cat(_ty, dim=0)\n",
        "            if to_cuda:\n",
        "                b_tx, b_ty = b_tx.cuda(), b_ty.cuda()\n",
        "            batched_tdata.append((b_ty, b_tx))\n",
        "            curr_batch = []\n",
        "    if len(curr_batch) > 0:\n",
        "        _tx, _ty = zip(*curr_batch)\n",
        "        b_tx = torch.cat(_tx, dim=0)\n",
        "        b_ty = torch.cat(_ty, dim=0)\n",
        "        if to_cuda:\n",
        "            b_tx, b_ty = b_tx.cuda(), b_ty.cuda()\n",
        "        batched_tdata.append((b_ty, b_tx))\n",
        "    return batched_tdata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vupBGBtP06Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window=0\n",
        "batched_train_0 = batchify(train[0], train[1], 2000, label_dict, window, True)\n",
        "batched_dev_0 = batchify(dev[0], dev[1], 2000, label_dict, window, True)\n",
        "batched_test_0 = batchify(test[0], test[1], 2000, label_dict, window, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBq9IrmQnmAP",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ePRzch10gII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP_Simple(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_size,\n",
        "                 num_labels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer0 = torch.nn.Linear(1 * 39, hidden_size)\n",
        "\n",
        "        self.layer1 = torch.nn.ReLU(inplace=False)\n",
        "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = torch.nn.ReLU(inplace=False)\n",
        "        self.layer4 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.final_layer = torch.nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Generate output distribution and argmax\n",
        "        Args:\n",
        "            x: num frames of 39-dimensional feature vector (MFCC features)\n",
        "        Return:\n",
        "            dist: log probability for each output class\n",
        "            pred: the label with highest log probability\n",
        "        \"\"\"\n",
        "        batch_size, frames, features = x.shape\n",
        "        x = x.squeeze(1)\n",
        "        x = self.layer0(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        y_hat = self.final_layer(x) \n",
        "        _, y_pred = y_hat.max(dim=-1) \n",
        "        return y_hat, y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVg6n6LTN9-F",
        "colab_type": "code",
        "outputId": "bfef2dee-84eb-43c6-9776-4585a505adfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_simple = MLP_Simple(hidden_size=300, num_labels=len(label_dict))\n",
        "print(model_simple)\n",
        "print('num parameters:', sum([p.numel() for p in model_simple.parameters()]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP_Simple(\n",
            "  (layer0): Linear(in_features=39, out_features=300, bias=True)\n",
            "  (layer1): ReLU()\n",
            "  (layer2): Linear(in_features=300, out_features=300, bias=True)\n",
            "  (layer3): ReLU()\n",
            "  (layer4): Linear(in_features=300, out_features=300, bias=True)\n",
            "  (final_layer): Linear(in_features=300, out_features=48, bias=True)\n",
            ")\n",
            "num parameters: 207048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0lS_yUqnyAp",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tpB--OO4zr0",
        "colab_type": "code",
        "outputId": "a79dca42-d032-475d-b07f-5981b3a22c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train_model(model, batched_train, batched_dev, batched_test, max_epoch=20):\n",
        "    model = model.cuda()\n",
        "    loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "    optim = torch.optim.Adam(model.parameters())\n",
        "  \n",
        "    for epoch in range(max_epoch):\n",
        "        random.shuffle(batched_train)\n",
        "        train_loss = []\n",
        "        train_acc = []\n",
        "        model.train()\n",
        "        for batch in batched_train:\n",
        "            optim.zero_grad()\n",
        "            y, x = batch\n",
        "            y_hat, y_pred = model(x)\n",
        "            batch_loss = loss(y_hat, y.long())\n",
        "            batch_loss.backward()\n",
        "            optim.step()\n",
        "            batch_acc = (y_pred == y.long()).sum().item() / y.numel()\n",
        "            train_loss.append(batch_loss.item())\n",
        "            train_acc.append(batch_acc)\n",
        "        _loss = sum(train_loss) / len(train_loss)\n",
        "        _acc = sum(train_acc) / len(train_acc)\n",
        "        print(f\"Epoch {epoch}\")\n",
        "        print(f\"train loss {_loss:.4f} train_acc {_acc:.4f}\")\n",
        "        dev_acc = []\n",
        "        model.eval()\n",
        "        for batch in batched_dev:\n",
        "            y, x = batch\n",
        "            with torch.no_grad():\n",
        "                y_hat, y_pred = model(x)\n",
        "                batch_acc = (y_pred == y.long()).sum().item() / y.numel()\n",
        "                dev_acc.append(batch_acc)\n",
        "        _acc = sum(dev_acc) / len(dev_acc)\n",
        "        print(f\"dev_acc {_acc:.4f}\")\n",
        "    test_acc = []\n",
        "    model.eval()\n",
        "    for batch in batched_test:\n",
        "        y, x = batch\n",
        "        with torch.no_grad():\n",
        "            y_hat, y_pred = model(x)\n",
        "            batch_acc = (y_pred == y.long()).sum().item() / y.numel()\n",
        "            test_acc.append(batch_acc)\n",
        "    _acc = sum(test_acc) / len(test_acc)\n",
        "    print(f\"training completed.\\n\")\n",
        "    print(f\"test_acc {_acc:.4f}\")\n",
        "\n",
        "train_model(model_simple, batched_train_0, batched_dev_0, batched_test_0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "train loss 1.9897 train_acc 0.4238\n",
            "dev_acc 0.4687\n",
            "Epoch 1\n",
            "train loss 1.5915 train_acc 0.5111\n",
            "dev_acc 0.5212\n",
            "Epoch 2\n",
            "train loss 1.4451 train_acc 0.5498\n",
            "dev_acc 0.5387\n",
            "Epoch 3\n",
            "train loss 1.3641 train_acc 0.5699\n",
            "dev_acc 0.5397\n",
            "Epoch 4\n",
            "train loss 1.3005 train_acc 0.5871\n",
            "dev_acc 0.5488\n",
            "Epoch 5\n",
            "train loss 1.2546 train_acc 0.5979\n",
            "dev_acc 0.5606\n",
            "Epoch 6\n",
            "train loss 1.2178 train_acc 0.6073\n",
            "dev_acc 0.5691\n",
            "Epoch 7\n",
            "train loss 1.1865 train_acc 0.6171\n",
            "dev_acc 0.5738\n",
            "Epoch 8\n",
            "train loss 1.1580 train_acc 0.6248\n",
            "dev_acc 0.5743\n",
            "Epoch 9\n",
            "train loss 1.1367 train_acc 0.6295\n",
            "dev_acc 0.5880\n",
            "Epoch 10\n",
            "train loss 1.1037 train_acc 0.6400\n",
            "dev_acc 0.5808\n",
            "Epoch 11\n",
            "train loss 1.0829 train_acc 0.6455\n",
            "dev_acc 0.5805\n",
            "Epoch 12\n",
            "train loss 1.0747 train_acc 0.6468\n",
            "dev_acc 0.5820\n",
            "Epoch 13\n",
            "train loss 1.0568 train_acc 0.6529\n",
            "dev_acc 0.5957\n",
            "Epoch 14\n",
            "train loss 1.0374 train_acc 0.6588\n",
            "dev_acc 0.5958\n",
            "Epoch 15\n",
            "train loss 1.0225 train_acc 0.6622\n",
            "dev_acc 0.5813\n",
            "Epoch 16\n",
            "train loss 1.0072 train_acc 0.6676\n",
            "dev_acc 0.5953\n",
            "Epoch 17\n",
            "train loss 0.9935 train_acc 0.6698\n",
            "dev_acc 0.5921\n",
            "Epoch 18\n",
            "train loss 0.9781 train_acc 0.6749\n",
            "dev_acc 0.5862\n",
            "Epoch 19\n",
            "train loss 0.9630 train_acc 0.6799\n",
            "dev_acc 0.5897\n",
            "training completed.\n",
            "\n",
            "test_acc 0.6271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0JrP5z0oG_G",
        "colab_type": "text"
      },
      "source": [
        "# Adding Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJRf2MyroRXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window=5\n",
        "batched_train_5 = batchify(train[0], train[1], 2000, label_dict, window, True)\n",
        "batched_dev_5 = batchify(dev[0], dev[1], 2000, label_dict, window, True)\n",
        "batched_test_5 = batchify(test[0], test[1], 2000, label_dict, window, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6lbbZ4Aon05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP_Context(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_size,\n",
        "                 num_labels):\n",
        "        super().__init__()\n",
        "        self.layer0 = torch.nn.Linear(11 * 39, hidden_size)\n",
        "\n",
        "        self.layer1 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer2 = torch.nn.ReLU(inplace=False)\n",
        "\n",
        "        self.final_layer = torch.nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Generate output distribution and argmax\n",
        "        Args:\n",
        "            x: num frames of 39-dimensional feature vector (MFCC features)\n",
        "        Return:\n",
        "            dist: log probability for each output class\n",
        "            pred: the label with highest log probability\n",
        "        \"\"\"\n",
        "        batch_size, frames, features = x.shape\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.layer0(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "\n",
        "        y_hat = self.final_layer(x)\n",
        "        _, y_pred = y_hat.max(dim=-1)\n",
        "        return y_hat, y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM2PBTqKpLE4",
        "colab_type": "code",
        "outputId": "a5c501b7-932d-49c3-bdc0-921aec4e19d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_context = MLP_Context(hidden_size=300, num_labels=len(label_dict))\n",
        "train_model(model_context, batched_train_5, batched_dev_5, batched_test_5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "train loss 1.7077 train_acc 0.5045\n",
            "dev_acc 0.5544\n",
            "Epoch 1\n",
            "train loss 1.3087 train_acc 0.5927\n",
            "dev_acc 0.5938\n",
            "Epoch 2\n",
            "train loss 1.1920 train_acc 0.6260\n",
            "dev_acc 0.6018\n",
            "Epoch 3\n",
            "train loss 1.1221 train_acc 0.6440\n",
            "dev_acc 0.5986\n",
            "Epoch 4\n",
            "train loss 1.0798 train_acc 0.6553\n",
            "dev_acc 0.6054\n",
            "Epoch 5\n",
            "train loss 1.0509 train_acc 0.6634\n",
            "dev_acc 0.6077\n",
            "Epoch 6\n",
            "train loss 1.0229 train_acc 0.6730\n",
            "dev_acc 0.6185\n",
            "Epoch 7\n",
            "train loss 0.9916 train_acc 0.6806\n",
            "dev_acc 0.6230\n",
            "Epoch 8\n",
            "train loss 0.9730 train_acc 0.6861\n",
            "dev_acc 0.6144\n",
            "Epoch 9\n",
            "train loss 0.9606 train_acc 0.6899\n",
            "dev_acc 0.6238\n",
            "Epoch 10\n",
            "train loss 0.9460 train_acc 0.6946\n",
            "dev_acc 0.6193\n",
            "Epoch 11\n",
            "train loss 0.9338 train_acc 0.6974\n",
            "dev_acc 0.6397\n",
            "Epoch 12\n",
            "train loss 0.9243 train_acc 0.7011\n",
            "dev_acc 0.6183\n",
            "Epoch 13\n",
            "train loss 0.9107 train_acc 0.7029\n",
            "dev_acc 0.6092\n",
            "Epoch 14\n",
            "train loss 0.8853 train_acc 0.7112\n",
            "dev_acc 0.6262\n",
            "Epoch 15\n",
            "train loss 0.8871 train_acc 0.7087\n",
            "dev_acc 0.6234\n",
            "Epoch 16\n",
            "train loss 0.8839 train_acc 0.7121\n",
            "dev_acc 0.6188\n",
            "Epoch 17\n",
            "train loss 0.8669 train_acc 0.7170\n",
            "dev_acc 0.6075\n",
            "Epoch 18\n",
            "train loss 0.8585 train_acc 0.7185\n",
            "dev_acc 0.6223\n",
            "Epoch 19\n",
            "train loss 0.8500 train_acc 0.7216\n",
            "dev_acc 0.6286\n",
            "training completed.\n",
            "\n",
            "test_acc 0.6587\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}