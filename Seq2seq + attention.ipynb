{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3 seq2seq_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzrok_oa0Jm4",
        "colab_type": "text"
      },
      "source": [
        "<center> <h1> Sequence-to-Sequence Models </h1> </center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvWbEBc7Oznn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-seq2seq-hw/cmudict.{dev,train,test}.src -q -nc\n",
        "!wget https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-seq2seq-hw/cmudict.{dev,train,test}.tgt -q -nc\n",
        "!wget https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-seq2seq-hw/cmudict.small.train.src -q -nc\n",
        "!wget https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-seq2seq-hw/cmudict.small.train.tgt -q -nc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTkWQn5qgh96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pr -m -t cmudict.small.train.src cmudict.small.train.tgt | head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3hJ1uofPbbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import time\n",
        "import editdistance as ed\n",
        "random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "torch.cuda.set_device(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTwCifaU3bqe",
        "colab_type": "text"
      },
      "source": [
        "# Data Reader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAxMdLq6PrCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPL_SYMS = ['<BOS>', '<EOS>', '<UNK>']\n",
        "\n",
        "\n",
        "class ParallelCorpus(object):\n",
        "    def __init__(self,\n",
        "                 src_file, tgt_file,\n",
        "                 src_vocab=None, tgt_vocab=None):\n",
        "        self.src_vocab = self.make_vocab(src_file, src_vocab)\n",
        "        self.tgt_vocab = self.make_vocab(tgt_file, tgt_vocab)\n",
        "        self.src_idx2vocab = self.make_idx2vocab(self.src_vocab)\n",
        "        self.tgt_idx2vocab = self.make_idx2vocab(self.tgt_vocab)\n",
        "        self.src_data = self.numberize(src_file, self.src_vocab)\n",
        "        self.tgt_data = self.numberize(tgt_file, self.tgt_vocab) if tgt_file is not None else None\n",
        "        assert len(self.src_data) == len(self.tgt_data), 'Source and Target have unequal lengths!'\n",
        "        self.data_size = len(self.src_data)\n",
        "\n",
        "    def numberize(self, txt, vocab):\n",
        "        data = []\n",
        "        with open(txt, 'r', encoding='utf8') as corpus:\n",
        "            for l in corpus:\n",
        "                d = [vocab['<BOS>']] + [vocab.get(tok, vocab['<UNK>']) for tok in l.strip().split()] + [vocab['<EOS>']]\n",
        "                d = torch.Tensor(d).long()\n",
        "                d = d.unsqueeze(0) # shape = (1, N)\n",
        "                data.append((d, l.strip()))\n",
        "        return data\n",
        "\n",
        "    def make_idx2vocab(self, vocab):\n",
        "        if vocab is not None:\n",
        "            idx2vocab = {v: k for k, v in vocab.items()}\n",
        "            return idx2vocab\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def make_vocab(self, txt, vocab):\n",
        "        if vocab is None and txt is not None:\n",
        "            v = {i: idx for idx, i in enumerate(SPL_SYMS)}\n",
        "            with open(txt, 'r', encoding='utf8') as corpus:\n",
        "                for line in corpus:\n",
        "                    for token in line.strip().split():\n",
        "                        v[token] = v.get(token, len(v))\n",
        "            return v\n",
        "        else:\n",
        "            return vocab\n",
        "\n",
        "    def get(self, idx):\n",
        "        if self.tgt_data is not None:\n",
        "            return self.src_data[idx], self.tgt_data[idx]\n",
        "        else:\n",
        "            return self.src_data[idx], (None, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_EeFzWu38b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_corpus = ParallelCorpus('cmudict.small.train.src',\n",
        "                              'cmudict.small.train.tgt')\n",
        "dev_corpus = ParallelCorpus('cmudict.dev.src', \n",
        "                            'cmudict.dev.tgt',\n",
        "                            train_corpus.src_vocab, \n",
        "                            train_corpus.tgt_vocab)\n",
        "test_corpus = ParallelCorpus('cmudict.test.src',\n",
        "                             'cmudict.test.tgt',\n",
        "                             train_corpus.src_vocab, \n",
        "                             train_corpus.tgt_vocab)\n",
        "print(train_corpus.data_size, dev_corpus.data_size, test_corpus.data_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRiDaagEyQaJ",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq with LSTM Language Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awZbYYjYQBof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMLM(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "              vocab_size,\n",
        "              embedding_size,\n",
        "              hidden_size,\n",
        "              num_layers=1,\n",
        "              dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "    \n",
        "    self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "    self.rnn = torch.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    self.output = torch.nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    pass\n",
        "\n",
        "  def forward(self, x, init_hidden_state=None):\n",
        "    assert x.shape[0] == 1 \n",
        "  \n",
        "    emb = self.embedding(x)\n",
        "    \n",
        "    emb = self.dropout(emb) \n",
        "\n",
        "    if init_hidden_state is None:\n",
        "      h0 = torch.zeros(self.num_layers, 1, self.hidden_size)\n",
        "      h0 = h0.cuda()\n",
        "      c0 = torch.zeros(self.num_layers, 1, self.hidden_size)\n",
        "      c0 = c0.cuda()\n",
        "    else:\n",
        "      h0, c0 = init_hidden_state\n",
        "    hidden_states, (hn, hc) = self.rnn(emb, (h0, c0))\n",
        "    hidden_states = hidden_states.cuda()\n",
        "\n",
        "    final_hidden_state = hn\n",
        "    final_cell_state = hc\n",
        "    final_state = [final_hidden_state, final_cell_state]\n",
        "\n",
        "  \n",
        "    hidden_states = self.dropout(hidden_states) \n",
        "\n",
        "    output_dist = self.output(hidden_states)\n",
        "\n",
        "    return output_dist, hidden_states, final_state \n",
        "    \n",
        "  def generate(self, start_idx, end_idx, init_hidden_state=None, idx2vocab=None, max_len=50):\n",
        "\n",
        "    if init_hidden_state is None:\n",
        "      h = torch.zeros((self.num_layers, 1, self.hidden_size))\n",
        "      c = torch.zeros((self.num_layers, 1, self.hidden_size))\n",
        "      h = h.cuda() if self.embedding.weight.is_cuda else h\n",
        "      c = c.cuda() if self.embedding.weight.is_cuda else c\n",
        "    else:\n",
        "      h, c = init_hidden_state\n",
        "    \n",
        "    inp = torch.tensor([start_idx]).long().unsqueeze(0)\n",
        "    inp = inp.cuda() if self.embedding.weight.is_cuda else inp\n",
        "\n",
        "    out = []\n",
        "    for _ in range(max_len):\n",
        "      with torch.no_grad():\n",
        "        emb = self.embedding(inp)\n",
        "        o, (h, c) = self.rnn(emb, (h, c))\n",
        "        o_dist = self.output(o)\n",
        "        _, pred = o_dist.max(dim=2)\n",
        "        if pred.item() == end_idx:\n",
        "          break\n",
        "        out.append(pred.item() if idx2vocab is None else idx2vocab[pred.item()])\n",
        "        inp = pred\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG0PyrdTQKxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "              src_vocab_size,\n",
        "              tgt_vocab_size,\n",
        "              embedding_size,\n",
        "              hidden_size,\n",
        "              num_layers=1,\n",
        "              dropout=0.1,\n",
        "              max_grad_norm=5.0):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_layers = num_layers\n",
        "    self.max_grad_norm = max_grad_norm\n",
        "    self.encoder = LSTMLM(src_vocab_size, embedding_size, hidden_size, num_layers, dropout)\n",
        "    self.decoder = LSTMLM(tgt_vocab_size, embedding_size, hidden_size, num_layers, dropout)\n",
        "    self.log_smax = torch.nn.LogSoftmax(dim=-1)\n",
        "    self.loss = torch.nn.NLLLoss(reduction='mean', ignore_index=-1)\n",
        "    #We are going to package the optimizer inside this class\n",
        "    self.optimizer = torch.optim.Adam(self.parameters())\n",
        "\n",
        "  def train_step(self, x, y):\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    _loss, acc = self(x, y)\n",
        "    _loss.backward()\n",
        "    grad_norm = torch.nn.utils.clip_grad_norm_(self.parameters(),\n",
        "                                                self.max_grad_norm)\n",
        "\n",
        "    if math.isnan(grad_norm):\n",
        "      print('skipping update grad_norm is nan!')\n",
        "    else:\n",
        "      self.optimizer.step()\n",
        "    loss = _loss.item()\n",
        "    return loss, acc\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    out_src, hidden_src, final_src = self.encoder(x)\n",
        "    \n",
        "    y_input = y[:, :-1]\n",
        "    y_output = y[:, 1:]\n",
        "\n",
        "    out_tgt, hidden_tgt, final_tgt = self.decoder(y_input, final_src)\n",
        "    out_tgt_lsm = self.log_smax(out_tgt)\n",
        "    \n",
        "    \n",
        "    loss = self.loss(out_tgt_lsm.squeeze(0), y_output.squeeze(0))\n",
        "    _, pred = out_tgt_lsm.max(dim=2)\n",
        "    accuracy = float((pred == y_output).sum()) / y_output.numel()\n",
        "    return loss, accuracy\n",
        "\n",
        "  def generate(self, x, start_idx, end_idx, idx2vocab=None, max_len=50):\n",
        "    _, _, final_src = self.encoder(x)\n",
        "    out = self.decoder.generate(start_idx, end_idx, final_src, idx2vocab)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITDCTNe_BPBC",
        "colab_type": "text"
      },
      "source": [
        "We create an instance of our EncoderDecoder below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyFZ3r53QQVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EncoderDecoder(src_vocab_size=len(train_corpus.src_vocab),\n",
        "                       tgt_vocab_size=len(train_corpus.tgt_vocab),\n",
        "                       embedding_size=73,\n",
        "                       hidden_size=73,\n",
        "                       num_layers=1)\n",
        "model = model.cuda()\n",
        "print(model)\n",
        "print('num parameters:', sum([p.numel() for p in model.parameters()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdDcLLPp-FVw",
        "colab_type": "text"
      },
      "source": [
        "## Training routine\n",
        "\n",
        "The `train` method defines our training routine. For the first seq2seq model `max_epochs` should be at least 15. For the second model, max_epochs can be reduced to 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0IFrAitSDDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_corpus, dev_corpus, max_epochs=15):\n",
        "  sum_loss, sum_acc = 0., 0.\n",
        "  train_instances_idxs = list(range(train_corpus.data_size))\n",
        "  st = time.time()\n",
        "  for epoch_i in range(max_epochs):\n",
        "    sum_loss, sum_acc = 0., 0.\n",
        "    random.shuffle(train_instances_idxs)\n",
        "    model.train()\n",
        "    for i in train_instances_idxs:\n",
        "      (x, _), (y, _) = train_corpus.get(i)\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "      l, a = model.train_step(x, y)\n",
        "      sum_loss += l\n",
        "      sum_acc += a\n",
        "    print(f\"epoch: {epoch_i} time elapsed: {time.time() - st:.2f}\")\n",
        "    print(f\"train loss: {sum_loss/train_corpus.data_size:.4f} train acc: {sum_acc/train_corpus.data_size:.4f}\")\n",
        "    sum_loss, sum_acc = 0., 0.\n",
        "    model.eval()\n",
        "    for dev_i in range(dev_corpus.data_size):\n",
        "      (x, x_str), (y, y_str) = dev_corpus.get(dev_i)\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "      with torch.no_grad():\n",
        "        l, a = model(x, y)\n",
        "        sum_loss += l.item()\n",
        "        sum_acc += a\n",
        "    print(f\"  dev loss: {sum_loss/dev_corpus.data_size:.4f}   dev acc: {sum_acc/dev_corpus.data_size:.4f}\")\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_khDY_-aOP",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation Routine\n",
        "We are going to evaluate our model's predictions using Character Error Rate (CER). This measures the number of edits (insertions, deletions and substitutions) needed to convert our model's prediction to the correct output sequence. The method before computes and prints the CER for each word in the test set and reports the average CER for the entire test set in the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6iRflipe9r9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, test_corpus):\n",
        "  print('Evaluation:')\n",
        "  sum_cer = 0.0\n",
        "  model.eval()\n",
        "  for test_i in range(test_corpus.data_size):\n",
        "    (x, x_str), (y, y_str) = test_corpus.get(test_i)\n",
        "    x, y = x.cuda(), y.cuda()\n",
        "    pred_seq = model.generate(x,\n",
        "                              test_corpus.tgt_vocab[SPL_SYMS[0]],\n",
        "                              test_corpus.tgt_vocab[SPL_SYMS[1]],\n",
        "                              test_corpus.tgt_idx2vocab)\n",
        "    cer = float(ed.eval(y_str.split(), pred_seq)) / len(y_str.split())\n",
        "    y_hat = ' '.join(pred_seq)\n",
        "    x_str = ''.join(x_str.split())\n",
        "    print(f\"{test_i} {x_str} pred: {y_hat} ref: {y_str} cer: {cer:.4f}\")\n",
        "    sum_cer += cer\n",
        "  print(f\"Avg CER: {sum_cer/test_corpus.data_size:.4f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv7USqUg-7Qe",
        "colab_type": "text"
      },
      "source": [
        "## Training the EncoderDecoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P138DwghPHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = train(model, train_corpus, dev_corpus) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv1HhJnhfga_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate(model, test_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLBaHKV8vVPC",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03kisWaevUKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "                hidden_size):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    m = torch.Tensor(hidden_size, hidden_size)\n",
        "    torch.nn.init.xavier_uniform_(m)\n",
        "    self.attn_weight_matrix = torch.nn.Parameter(m)\n",
        "      \n",
        "  def forward(self, encoder_states, prev_decoder_state):\n",
        "\n",
        "    prev_decoder_state = prev_decoder_state.view(self.hidden_size, 1)\n",
        "\n",
        "    attn_wts = torch.mm(self.attn_weight_matrix, prev_decoder_state)\n",
        "    \n",
        "    attn_wts = attn_wts/math.sqrt(64)\n",
        "\n",
        "    self.softm = torch.nn.LogSoftmax(dim=0)\n",
        "    attn_probs = self.softm(attn_wts)\n",
        "\n",
        "    context_vector = attn_probs * torch.sum(encoder_states)\n",
        "    \n",
        "    context_vector = context_vector.unsqueeze(1) \n",
        "    return context_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_05GsaUv4yB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionDecoder(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "                vocab_size,\n",
        "                embedding_size,\n",
        "                hidden_size,\n",
        "                num_layers=1,\n",
        "                dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout_prop = dropout\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding = torch.nn.Embedding(self.vocab_size, self.embedding_size)\n",
        "    self.output_proj = torch.nn.Linear(self.hidden_size, self.vocab_size)\n",
        "    self.rnn = torch.nn.LSTM(embedding_size + hidden_size, hidden_size, num_layers,\n",
        "                              batch_first=True, dropout=dropout, bidirectional=False)\n",
        "    self.attention = Attention(self.hidden_size)\n",
        "\n",
        "  def forward(self, encoder_states, y):\n",
        "\n",
        "    h, c = (torch.zeros(self.num_layers, 1, self.hidden_size),\n",
        "            torch.zeros(self.num_layers, 1, self.hidden_size))\n",
        "\n",
        "    tgt_embedding = self.embedding(y[0, 0]).view(1, self.embedding_size)\n",
        "\n",
        "    output_buffer = []\n",
        "    for tgt_idx in range(y.shape[1] - 1):\n",
        "      context_vector = self.attention(encoder_states, h).view(1, -1)\n",
        "\n",
        "      decoder_input = torch.cat((tgt_embedding, context_vector), -1).unsqueeze(0)\n",
        "\n",
        "      o, (h, c) = self.rnn(decoder_input, (h, c))\n",
        "\n",
        "      output = self.output_proj(o).view(1, 1, self.vocab_size)\n",
        "      \n",
        "      output_buffer.append(output)\n",
        "      tgt = y[:, tgt_idx + 1]\n",
        "      tgt_embedding = self.embedding(tgt)\n",
        "    output_dist = torch.cat(output_buffer, dim=1)\n",
        "    return output_dist\n",
        "\n",
        "  def generate(self, encoder_states, start_idx, end_idx, idx2vocab=None, max_len=50):\n",
        "\n",
        "    h, c = (torch.zeros(self.num_layers, 1, self.hidden_size).type_as(encoder_states),\n",
        "            torch.zeros(self.num_layers, 1, self.hidden_size).type_as(encoder_states))\n",
        "    \n",
        "    inp = torch.tensor(start_idx).view(1, 1)\n",
        "    inp = inp.cuda()\n",
        "\n",
        "\n",
        "    out = [] \n",
        "    for _ in range(max_len):\n",
        "\n",
        "      tgt_embedding = self.embedding(inp).view(1, self.embedding_size)\n",
        "\n",
        "      context_vector = self.attention(encoder_states, h).view(1, -1)\n",
        "\n",
        "      decoder_input = torch.cat((context_vector, tgt_embedding), -1 ).unsqueeze(0)\n",
        "      o, (h, c) = self.rnn(decoder_input, (h, c))\n",
        "\n",
        "      o_dist = self.output_proj(o).view(1, 1, self.vocab_size)\n",
        "\n",
        "      max_idxa = torch.argmax(o_dist)\n",
        "      max_idx = max_idxa.item()\n",
        " \n",
        "      if max_idx == end_idx:\n",
        "        break\n",
        "      else:\n",
        "        idx2vocab = str(max_idx)\n",
        "        out.append(idx2vocab)\n",
        "      inp = torch.tensor(max_idx)\n",
        "      inp = inp.cuda()\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cVH_zFg0-qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoderAttention(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "                src_vocab_size,\n",
        "                tgt_vocab_size,\n",
        "                embedding_size,\n",
        "                hidden_size,\n",
        "                num_layers=1,\n",
        "                dropout=0.0,\n",
        "                max_grad_norm=5.0):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_layers = num_layers\n",
        "    self.max_grad_norm = max_grad_norm\n",
        "    self.encoder = LSTMLM(src_vocab_size, embedding_size, hidden_size, num_layers, dropout)\n",
        "    self.decoder = AttentionDecoder(tgt_vocab_size, embedding_size, hidden_size, num_layers, dropout)\n",
        "    self.log_smax = torch.nn.LogSoftmax(dim=-1)\n",
        "    self.loss = torch.nn.NLLLoss(reduction='mean', ignore_index=-1)\n",
        "    self.optimizer = torch.optim.Adam(self.parameters())\n",
        "\n",
        "  def train_step(self, x, y):\n",
        "    self.optimizer.zero_grad()\n",
        "    _loss, acc = self(x, y)\n",
        "    _loss.backward()\n",
        "    grad_norm = torch.nn.utils.clip_grad_norm_(self.parameters(),\n",
        "                                                self.max_grad_norm)\n",
        "\n",
        "    if math.isnan(grad_norm):\n",
        "      print('skipping update grad_norm is nan!')\n",
        "    else:\n",
        "      self.optimizer.step()\n",
        "    loss = _loss.item()\n",
        "    return loss, acc\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    _, encoder_states, _ = self.encoder(x)\n",
        "    out_tgt = self.decoder(encoder_states, y)\n",
        "    out_tgt_lsm = self.log_smax(out_tgt)\n",
        "    y_output = y[:, 1:]\n",
        "    loss = self.loss(out_tgt_lsm.squeeze(0), y_output.squeeze(0))\n",
        "    _, pred = out_tgt.max(dim=2)\n",
        "    accuracy = float((pred == y_output).sum()) / y_output.numel()\n",
        "    return loss, accuracy\n",
        "\n",
        "  def generate(self, x, start_idx, end_idx, idx2vocab=None, max_len=50):\n",
        "    _, encoder_states, _ = self.encoder(x)\n",
        "    out = self.decoder.generate(encoder_states, start_idx, end_idx, idx2vocab)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsdq5VC-2ri9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_corpus.data_size, dev_corpus.data_size, test_corpus.data_size)\n",
        "model_attn = EncoderDecoderAttention(src_vocab_size=len(train_corpus.src_vocab),\n",
        "                                tgt_vocab_size=len(train_corpus.tgt_vocab),\n",
        "                                embedding_size=64,\n",
        "                                hidden_size=64,\n",
        "                                num_layers=1)\n",
        "model_attn = model_attn.cuda()\n",
        "print(model_attn)\n",
        "print('num parameters:', sum([p.numel() for p in model_attn.parameters()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3KS9Wjb21gV",
        "colab_type": "code",
        "outputId": "0c165a39-630d-4337-954d-389769056538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_attn = train(model_attn, train_corpus, dev_corpus, max_epochs=10) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3a6de9a415c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#takes 1 - 1.5 hours\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#this is the log from the first 3 epochs for me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f1628242011c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_corpus, dev_corpus, max_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0msum_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-cbff1f8ee72b>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \"\"\"\n\u001b[1;32m     31\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0m_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0m_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     grad_norm = torch.nn.utils.clip_grad_norm_(self.parameters(),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-cbff1f8ee72b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \"\"\"\n\u001b[1;32m     53\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mout_tgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mout_tgt_lsm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_smax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0my_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-32879a3dcbd6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_states, y)\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0;31m#TODO: compute the `context_vector` using self.attention object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0;31m#TODO: `self.attention` takes `encoder_states` and the decoder hidden_state `h` as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m       \u001b[0mcontext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m       \u001b[0;31m# print(context_vector.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-2ecb75362e85>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_states, prev_decoder_state)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#TODO: name the result of the two matrix multiplcations as `attn_wts`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#TODO: the shape of `attn_wts` should be (src_length, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mattn_wts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_weight_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_decoder_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mattn_wts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_wts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #2 'mat2' in call to _th_mm"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7P8WaShfugy",
        "colab_type": "code",
        "outputId": "f5468f3e-e978-42c1-9b8f-1d8aff69ff51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "evaluate(model_attn, test_corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation:\n",
            "0 vanlaningham pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: V AE2 N L AE1 N IH0 NG HH AE2 M cer: 4.5455\n",
            "1 utility's pred: 55 35 35 35 15 16 35 15 16 35 15 16 35 15 16 15 16 35 15 16 15 16 15 16 35 15 16 15 16 35 15 16 15 16 15 16 35 15 16 15 16 15 16 35 15 16 15 16 15 16 ref: Y UW0 T IH1 L AH0 T IY0 Z cer: 5.5556\n",
            "2 rothenberg pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: R AO1 TH AH0 N B ER0 G cer: 6.2500\n",
            "3 kinesiology pred: 35 73 21 35 73 41 35 73 41 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 ref: K IH2 N IH0 S IY2 AA1 L AH0 JH IY0 cer: 4.5455\n",
            "4 reclassified pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: R IY0 K L AE1 S AH0 F AY2 D cer: 5.0000\n",
            "5 substantive pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: S AH1 B S T AH0 N T IH0 V cer: 5.0000\n",
            "6 situations pred: 35 73 21 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 15 35 15 35 15 21 35 15 55 35 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 ref: S IH2 CH UW0 EY1 SH AH0 N Z cer: 5.5556\n",
            "7 lobianco pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: L OW0 B IY0 AA1 N K OW0 cer: 6.2500\n",
            "8 participants' pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: P AA0 R T IH1 S AH0 P AH0 N T S cer: 4.1667\n",
            "9 transportable pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: T R AE0 N S P AO1 R T AH0 B AH0 L cer: 3.8462\n",
            "10 inscribes pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: IH2 N S K R AY1 B Z cer: 6.2500\n",
            "11 grandbaby pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: G R AE1 N D B EY2 B IY0 cer: 5.5556\n",
            "12 costanzo pred: 35 35 73 21 35 35 35 35 35 15 16 35 15 16 35 15 55 15 55 15 55 15 55 15 55 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 15 16 ref: K OW0 S T AA1 N Z OW0 cer: 6.2500\n",
            "13 novakovich pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 ref: N AH0 V AA1 K AH0 V IH0 CH cer: 5.5556\n",
            "14 engulfing pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: IH0 N G AH1 L F IH0 NG cer: 6.2500\n",
            "15 jorgenson pred: 35 73 41 35 73 41 35 73 41 35 73 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 ref: JH AO1 R G IH0 N S AH0 N cer: 5.5556\n",
            "16 marketeers pred: 35 21 73 41 35 73 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 ref: M AA2 R K AH0 T IH1 R Z cer: 5.5556\n",
            "17 branagan pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: B R AE1 N AH0 G AE0 N cer: 6.2500\n",
            "18 toshimitsu pred: 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ref: T OW0 SH IY0 M IY1 T S UW0 cer: 5.5556\n",
            "19 electrobiology pred: 3 57 18 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 18 54 ref: IH2 L EH2 K T R OW0 B AY0 AA1 L AH0 JH IY2 cer: 3.5714\n",
            "20 flagships pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: F L AE1 G SH IH2 P S cer: 6.2500\n",
            "21 downsizing pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 ref: D AW1 N S AY2 Z IH0 NG cer: 6.2500\n",
            "22 biofeedback pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: B AY0 OW0 F IY1 D B AE2 K cer: 5.5556\n",
            "23 retrovir pred: 55 35 35 35 35 21 35 35 15 16 35 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 16 15 ref: R EH1 T R OW0 V IH2 R cer: 6.2500\n",
            "24 jovanovic pred: 55 55 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 0 15 16 35 35 ref: Y AH0 V AH0 N AA1 V IH0 K cer: 5.5556\n",
            "25 conjugated pred: 35 73 21 35 35 73 41 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 15 35 15 35 15 35 15 55 35 15 55 15 55 15 55 15 55 15 55 15 55 15 ref: K AA2 N JH AH0 G EY1 T IH0 D cer: 5.0000\n",
            "26 brooksville pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: B R UH1 K S V IH0 L cer: 6.2500\n",
            "27 nonperforming pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: N AA0 N P ER0 F AO1 R M IH0 NG cer: 4.5455\n",
            "28 fulfilling pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: F UH0 L F IH1 L IH0 NG cer: 6.2500\n",
            "29 conjecture pred: 49 57 57 18 49 18 49 57 18 49 18 57 57 18 49 18 49 18 57 57 18 49 18 49 18 57 18 49 18 57 18 49 18 57 18 49 18 57 18 49 18 57 18 49 18 49 18 57 57 18 ref: K AH0 N JH EH1 K CH ER0 cer: 6.2500\n",
            "30 adjustments pred: 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ref: AH0 JH AH1 S T M AH0 N T S cer: 5.0000\n",
            "31 hoglund's pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: HH AO1 G L AH0 N D Z cer: 6.2500\n",
            "32 languished pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: L AE1 NG G W IH0 SH T cer: 6.2500\n",
            "33 obscenity pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: AH0 B S EH1 N IH0 T IY0 cer: 6.2500\n",
            "34 kingsport pred: 3 3 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 ref: K IH1 NG S P AO2 R T cer: 6.2500\n",
            "35 premonitory pred: 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ref: P R AH0 M AH1 N AH0 T ER0 IY0 cer: 5.0000\n",
            "36 slowinski pred: 49 57 57 18 49 18 49 57 18 49 18 57 57 18 49 18 49 18 57 57 18 49 18 49 18 57 18 49 18 57 18 49 18 49 18 57 57 18 49 18 49 18 57 18 49 18 57 18 49 18 ref: S L OW0 IH1 N S K IY0 cer: 6.2500\n",
            "37 schiltknecht pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: SH IH1 L T N EH2 K T cer: 6.2500\n",
            "38 stolichnaya pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: S T OW2 L IH0 K N AY1 AH0 cer: 5.5556\n",
            "39 subfamily pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: S AH1 B F AE2 M AH0 L IY0 cer: 5.5556\n",
            "40 peterbilt pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: P IY1 T ER0 B IH2 L T cer: 6.2500\n",
            "41 mcdermid pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: M AH0 K D ER1 M AH0 D cer: 6.2500\n",
            "42 simulcast pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: S AY1 M Y AH0 L K AE2 S T cer: 5.0000\n",
            "43 reemergence pred: 22 22 12 12 12 15 55 55 55 55 55 2 35 35 11 35 35 11 35 15 35 11 35 35 11 35 15 35 11 35 35 11 35 15 35 11 35 35 11 35 15 35 11 35 35 11 35 15 35 11 ref: R IY0 IH0 M ER1 JH AH0 N S cer: 5.5556\n",
            "44 papadopoulos pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: P AE2 P AH0 D AA1 P AH0 L AH0 S cer: 4.5455\n",
            "45 factually pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: F AE1 K CH UW0 AH0 L IY0 cer: 6.2500\n",
            "46 hildegarde pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: HH IH1 L D IH0 G AA2 R D cer: 5.5556\n",
            "47 gingerbread pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: JH IH1 N JH ER0 B R EH2 D cer: 5.5556\n",
            "48 rosenman pred: 49 49 57 57 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 59 57 18 49 18 49 18 49 18 49 18 49 18 49 18 59 57 18 49 18 49 18 49 18 49 ref: R OW1 Z AH0 N M AH0 N cer: 6.2500\n",
            "49 unabated pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 ref: AH2 N AH0 B EY1 T IH0 D cer: 6.2500\n",
            "50 batignolles pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: B AE2 T IH0 N Y OW1 L AH0 S cer: 5.0000\n",
            "51 frazzini pred: 3 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 ref: F R AA0 T S IY1 N IY0 cer: 6.2500\n",
            "52 disapproving pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: D IH0 S AH0 P R UW1 V IH0 NG cer: 5.0000\n",
            "53 ventilating pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: V EH1 N T AH0 L EY2 T IH0 NG cer: 5.0000\n",
            "54 believability pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: B AH0 L IY2 V AH0 B IH1 L IH0 T IY0 cer: 4.1667\n",
            "55 punctuating pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: P AH1 NG K CH UW0 EY2 D IH0 NG cer: 5.0000\n",
            "56 countdowns pred: 49 49 57 57 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 59 57 18 49 18 49 18 49 18 49 18 49 18 49 18 59 57 18 49 18 49 18 49 ref: K AW1 N T D AW2 N Z cer: 6.2500\n",
            "57 legalities pred: 55 55 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 ref: L IY0 G AE1 L IH0 T IY0 Z cer: 5.5556\n",
            "58 intercellular pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: IH2 N T ER0 S EH1 L Y AH0 L ER0 cer: 4.5455\n",
            "59 extravaganza pred: 35 21 73 41 35 73 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 ref: EH0 K S T R AE2 V AH0 G AE1 N Z AH0 cer: 3.8462\n",
            "60 entombment pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: IH0 N T UW1 M M AH0 N T cer: 5.5556\n",
            "61 suspensions pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: S AH0 S P EH1 N SH AH0 N Z cer: 5.0000\n",
            "62 recognition's pred: 35 73 21 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 15 35 15 35 15 21 35 15 35 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 ref: R EH2 K IH0 G N IH1 SH AH0 N Z cer: 4.5455\n",
            "63 compartments pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: K AH0 M P AA1 R T M AH0 N T S cer: 4.1667\n",
            "64 triumphantly pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: T R AY0 AH1 M F AH0 N T L IY0 cer: 4.5455\n",
            "65 unreliable pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: AH2 N R IH0 L AY1 AH0 B AH0 L cer: 5.0000\n",
            "66 northwestern's pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 18 49 18 49 18 49 18 49 18 49 18 49 18 ref: N AO2 R TH W EH1 S T ER0 N Z cer: 4.5455\n",
            "67 hempfling pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: HH EH1 M P F AH0 L IH0 NG cer: 5.5556\n",
            "68 renegotiate pred: 21 21 21 21 21 21 35 21 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 ref: R IY2 N IH0 G OW1 SH IY0 EY2 T cer: 5.0000\n",
            "69 mandalit pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: M AE1 N D AH0 L IH2 T cer: 6.2500\n",
            "70 criscione pred: 49 57 57 18 49 18 49 18 57 57 18 49 18 49 18 49 18 57 57 18 49 18 49 18 49 18 57 57 18 49 18 49 18 49 18 57 18 49 18 49 18 57 18 49 18 49 18 57 18 49 ref: K R IY0 S CH OW1 N IY0 cer: 6.2500\n",
            "71 gilchrest pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: G IH1 L K ER0 IH0 S T cer: 6.2500\n",
            "72 interiors pred: 35 73 21 35 73 41 35 73 41 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 ref: IH2 N T IH1 R IY0 ER0 Z cer: 6.2500\n",
            "73 misdiagnosis pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 ref: M IH0 S D AY2 IH0 G N OW1 S AH0 S cer: 4.1667\n",
            "74 repositories pred: 35 73 21 35 35 35 35 35 35 35 35 35 35 35 35 35 35 15 35 15 21 35 15 35 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 ref: R IY0 P AA1 Z AH0 T AO2 R IY0 Z cer: 4.5455\n",
            "75 lithuanian pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: L IH2 TH AH0 W EY1 N IY0 AH0 N cer: 5.0000\n",
            "76 transcanada's pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: T R AE2 N Z K AE1 N AH0 D AH0 Z cer: 4.1667\n",
            "77 badalamenti pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: B AA0 D AA0 L AA0 M EH1 N T IY0 cer: 4.5455\n",
            "78 verifiable pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: V EH1 R AH0 F AY2 AH0 B AH0 L cer: 5.0000\n",
            "79 hempstead pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: HH EH1 M P S T EH0 D cer: 6.2500\n",
            "80 prerecorded pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: P R IY2 R IY0 K AO1 R D IH0 D cer: 4.5455\n",
            "81 clipboard pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: K L IH1 P B AO2 R D cer: 6.2500\n",
            "82 paganism pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: P EY1 G AH0 N IH2 Z AH0 M cer: 5.5556\n",
            "83 fabricate pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: F AE1 B R AH0 K EY2 T cer: 6.2500\n",
            "84 teleprobe pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: T EH1 L AH0 P R OW1 B cer: 6.2500\n",
            "85 cryogenic pred: 49 49 57 57 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 59 57 18 49 18 49 18 49 18 49 18 49 18 49 18 59 57 18 49 18 49 18 49 18 49 ref: K R AY1 AH0 JH EH2 N IH0 K cer: 5.5556\n",
            "86 unmanaged pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 18 49 18 49 18 49 18 49 18 49 18 49 ref: AH0 N M AE1 N IH0 JH D cer: 6.2500\n",
            "87 intricate pred: 35 73 21 35 35 35 35 35 35 35 35 35 35 15 16 35 15 35 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 15 55 ref: IH1 N T R AH0 K AH0 T cer: 6.2500\n",
            "88 familiarize pred: 3 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 ref: F AH0 M IH1 L Y ER0 AY2 Z cer: 5.5556\n",
            "89 st-thomas pred: 55 35 35 35 15 16 35 15 16 35 15 16 35 15 16 35 15 16 15 16 35 15 16 15 16 35 15 16 15 16 35 15 16 15 16 35 15 16 15 16 35 15 16 15 16 35 15 16 15 16 ref: S EY1 N T AA1 M AH0 S cer: 6.2500\n",
            "90 kredietbank pred: 3 49 18 57 57 18 57 57 18 49 18 57 57 18 49 18 57 57 18 49 18 57 57 18 49 18 57 57 18 49 18 57 57 18 30 18 49 18 57 57 18 30 18 49 18 57 57 18 30 18 ref: K R EH0 D IY0 T B AE1 NG K cer: 5.0000\n",
            "91 santa-fe's pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: S AE1 N T AH0 F EY1 Z cer: 6.2500\n",
            "92 distillate pred: 3 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 ref: D IH1 S T AH0 L EY2 T cer: 6.2500\n",
            "93 palkovic pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: P AH0 L K AA1 V IH0 K cer: 6.2500\n",
            "94 calculate pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: K AE1 L K Y AH0 L EY2 T cer: 5.5556\n",
            "95 plebeians pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: P L AH0 B IY1 AH0 N Z cer: 6.2500\n",
            "96 insubordination pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: IH2 N S AH0 B AO2 R D AH0 N EY1 SH AH0 N cer: 3.5714\n",
            "97 recommendations pred: 3 49 57 18 57 57 57 18 49 18 57 57 18 57 57 18 49 18 57 57 18 49 18 57 57 18 57 18 49 18 57 57 18 30 18 49 18 57 57 18 30 18 49 18 57 57 18 30 18 49 ref: R EH2 K AH0 M AH0 N D EY1 SH AH0 N Z cer: 3.8462\n",
            "98 galentine pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 ref: G AA0 L EH0 N T IY1 N IY0 cer: 5.5556\n",
            "99 comedienne pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: K AH0 M IY2 D IY0 EH1 N cer: 6.2500\n",
            "100 stolarski pred: 35 73 21 35 73 41 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 15 35 15 35 15 35 15 35 15 55 35 15 55 15 55 15 55 15 55 15 ref: S T AH0 L AA1 R S K IY2 cer: 5.5556\n",
            "101 reprocessed pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: R IY0 P R AO1 S EH0 S T cer: 5.5556\n",
            "102 d'artagnan pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 ref: D AH0 R T AE1 NG Y AH0 N # foreign french cer: 4.1667\n",
            "103 radiators pred: 35 21 21 73 41 35 21 73 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 ref: R EY1 D IY0 EY2 T ER0 Z cer: 6.2500\n",
            "104 rollerblading pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: R OW1 L ER0 B L EY2 D IH0 NG cer: 5.0000\n",
            "105 depended pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: D IH0 P EH1 N D AH0 D cer: 6.2500\n",
            "106 sardinia pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 49 18 ref: S AA0 R D IY1 N IY0 AH0 cer: 6.2500\n",
            "107 intending pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: IH2 N T EH1 N D IH0 NG cer: 6.2500\n",
            "108 homogenization pred: 21 21 21 21 21 21 21 21 21 35 21 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 ref: HH OW0 M AA1 JH AH0 N IH0 Z EY2 SH AH0 N cer: 3.8462\n",
            "109 telecasts pred: 3 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 ref: T EH1 L AH0 K AE2 S T S cer: 5.5556\n",
            "110 matriculating pred: 3 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 ref: M AH0 T R IH1 K Y AH0 L EY0 T IH0 NG cer: 3.8462\n",
            "111 conception pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: K AH0 N S EH1 P SH AH0 N cer: 5.5556\n",
            "112 simonson pred: 3 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 ref: S IH1 M AH0 N S AH0 N cer: 6.2500\n",
            "113 inoffensive pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: IH2 N AH0 F EH1 N S IH0 V cer: 5.5556\n",
            "114 nationalistic pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: N AE2 SH AH0 N AH0 L IH1 S T IH0 K cer: 4.1667\n",
            "115 fantastically pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: F AE0 N T AE1 S T IH0 K L IY0 cer: 4.5455\n",
            "116 istituto pred: 35 21 73 41 35 73 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 ref: IH2 S T IH0 T UW1 T OW0 cer: 6.2500\n",
            "117 florentino pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: F L AO0 R EH0 N T IY1 N OW0 cer: 5.0000\n",
            "118 microcircuit pred: 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ref: M AY1 K R OW0 S ER2 K AH0 T cer: 5.0000\n",
            "119 imperfectly pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: IH2 M P ER1 F IH0 K T L IY0 cer: 5.0000\n",
            "120 desegregation pred: 35 21 73 41 35 73 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 41 35 ref: D IH0 S EH2 G R AH0 G EY1 SH AH0 N cer: 4.1667\n",
            "121 fleischhacker pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: F L AY1 SH HH AH0 K ER0 cer: 6.2500\n",
            "122 dupriest pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: D AH1 P ER0 IY0 IH0 S T cer: 6.2500\n",
            "123 unattractive pred: 22 22 35 35 11 35 35 11 35 15 35 11 35 35 11 35 35 11 35 35 11 35 35 35 11 35 35 11 35 35 35 11 35 35 35 11 35 35 35 11 35 35 35 11 35 35 35 11 35 35 ref: AH2 N AH0 T R AE1 K T IH0 V cer: 5.0000\n",
            "124 sanhedrin pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: S AE2 N HH IY1 D R IH0 N cer: 5.5556\n",
            "125 callousness pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: K AE1 L AH0 S N AH0 S cer: 6.2500\n",
            "126 dermatologist pred: 35 73 21 73 41 35 73 41 35 35 41 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 ref: D ER2 M AH0 T AA1 L AH0 JH IH0 S T cer: 4.1667\n",
            "127 harmonics pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: HH AA0 R M AA1 N IH0 K S cer: 5.5556\n",
            "128 deployable pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: D IH0 P L OY1 AH0 B AH0 L cer: 5.5556\n",
            "129 approximately pred: 49 49 57 57 18 49 18 49 18 49 18 49 18 49 18 49 18 57 57 18 49 18 49 18 49 18 49 18 49 18 49 18 59 57 18 49 18 49 18 49 18 49 18 49 18 59 57 18 49 18 ref: AH0 P R AA1 K S AH0 M AH0 T L IY0 cer: 4.1667\n",
            "130 remerchandise pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: R IY0 M ER1 CH AH0 N D AY2 Z cer: 5.0000\n",
            "131 bianchini pred: 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 ref: B IY0 AA0 N CH IY1 N IY0 cer: 6.2500\n",
            "132 invigorating pred: 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ref: IH2 N V IH1 G ER0 EY2 T IH0 NG cer: 5.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5f932f0837b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-a82f01ce3db5>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, test_corpus)\u001b[0m\n\u001b[1;32m      9\u001b[0m                               \u001b[0mtest_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSPL_SYMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                               \u001b[0mtest_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSPL_SYMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                               test_corpus.tgt_idx2vocab)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mcer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0med\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-cbff1f8ee72b>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, x, start_idx, end_idx, idx2vocab, max_len)\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}